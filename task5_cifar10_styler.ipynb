{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "from tqdm import tqdm \n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms, models\n",
    "from models.model import ResNet50, ViTBase, CLIPModel\n",
    "from models.utils import train_step, eval_step, DataLoaders, CIFAR100_Splits\n",
    "from models.transforms import transforms_resnet,transforms_clip_vit,transforms_vit\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.datasets import CIFAR10, CIFAR100\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/vgg19-dcbb9e9d.pth\" to C:\\Users\\PCF/.cache\\torch\\hub\\checkpoints\\vgg19-dcbb9e9d.pth\n",
      "100%|██████████| 548M/548M [23:07<00:00, 414kB/s]   \n"
     ]
    }
   ],
   "source": [
    "# get the \"features\" portion of VGG19 (we will not need the \"classifier\" portion)\n",
    "vgg = models.vgg19(pretrained=True).features\n",
    "\n",
    "# freeze all VGG parameters since we're only optimizing the target image\n",
    "for param in vgg.parameters():\n",
    "    param.requires_grad_(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (1): ReLU(inplace=True)\n",
       "  (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (3): ReLU(inplace=True)\n",
       "  (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (6): ReLU(inplace=True)\n",
       "  (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (8): ReLU(inplace=True)\n",
       "  (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (11): ReLU(inplace=True)\n",
       "  (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (13): ReLU(inplace=True)\n",
       "  (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (15): ReLU(inplace=True)\n",
       "  (16): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (17): ReLU(inplace=True)\n",
       "  (18): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (19): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (20): ReLU(inplace=True)\n",
       "  (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (22): ReLU(inplace=True)\n",
       "  (23): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (24): ReLU(inplace=True)\n",
       "  (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (26): ReLU(inplace=True)\n",
       "  (27): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (29): ReLU(inplace=True)\n",
       "  (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (31): ReLU(inplace=True)\n",
       "  (32): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (33): ReLU(inplace=True)\n",
       "  (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (35): ReLU(inplace=True)\n",
       "  (36): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       ")"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(torch.cuda.is_available())\n",
    "vgg.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(img_path, max_size=400, shape=None):\n",
    "    ''' Load in and transform an image, making sure the image\n",
    "       is <= 400 pixels in the x-y dims.'''\n",
    "\n",
    "    image = Image.open(img_path).convert('RGB')\n",
    "\n",
    "    # large images will slow down processing\n",
    "    if max(image.size) > max_size:\n",
    "        size = max_size\n",
    "    else:\n",
    "        size = max(image.size)\n",
    "\n",
    "    if shape is not None:\n",
    "        size = shape\n",
    "\n",
    "    in_transform = transforms.Compose([\n",
    "                        transforms.Resize(size),\n",
    "                        transforms.ToTensor(),\n",
    "                        transforms.Normalize((0.485, 0.456, 0.406),\n",
    "                                             (0.229, 0.224, 0.225))])\n",
    "\n",
    "    # discard the transparent, alpha channel (that's the :3) and add the batch dimension\n",
    "    image = in_transform(image)[:3,:,:].unsqueeze(0)\n",
    "\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import random\n",
    "# Define a transform to normalize the data\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalize to [-1, 1]\n",
    "])\n",
    "\n",
    "# Load the CIFAR-10 training and test sets\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# Create data loaders\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True, num_workers=2)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False, num_workers=2)\n",
    "\n",
    "# Classes in CIFAR-10\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5  # Unnormalize to [0, 1]\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "# Fetch a random image from the dataset\n",
    "def get_random_image(trainset):\n",
    "    # Generate a random index\n",
    "    random_idx = random.randint(0, len(trainset) - 1)\n",
    "    # Fetch the image and its label\n",
    "    image, label = trainset[random_idx]\n",
    "    return image, label\n",
    "def load_image_from_pil(image, max_size=400, shape=None):\n",
    "    ''' Load in and transform a PIL Image, ensuring the image\n",
    "        is resized appropriately. '''\n",
    "\n",
    "    # Convert to RGB if not already\n",
    "    if image.mode != 'RGB':\n",
    "        image = image.convert('RGB')\n",
    "\n",
    "    # Determine the new size based on max_size\n",
    "    if max(image.size) > max_size:\n",
    "        size = max_size\n",
    "    else:\n",
    "        size = max(image.size)\n",
    "\n",
    "    if shape is not None:\n",
    "        size = shape\n",
    "\n",
    "    # Define the transformation\n",
    "    in_transform = transforms.Compose([\n",
    "        transforms.Resize(size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.485, 0.456, 0.406),\n",
    "                             (0.229, 0.224, 0.225))\n",
    "    ])\n",
    "\n",
    "    # Apply the transformation\n",
    "    image = in_transform(image)[:3, :, :].unsqueeze(0)  # Discard alpha channel and add batch dimension\n",
    "\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ship\n"
     ]
    }
   ],
   "source": [
    "image, label = get_random_image(trainset)\n",
    "\n",
    "# Show the random image\n",
    "imshow(image)\n",
    "print(classes[label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in content and style image\n",
    "content = image.to(device)\n",
    "# Resize style to match content, makes code easier\n",
    "#curl --output starry_night.jpg 'https://img.freepik.com/premium-photo/immerse-enigmatic-beauty-starry-night-art-wallpaper-by-vincent-van-gogh-ar-32-desktop_983420-139572.jpg' 127.0.0.1:8000\n",
    "style = load_image_from_pil(Image.open('starry_night.jpeg'), shape=content.shape[-2:]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def im_convert(tensor):\n",
    "    \"\"\" Display a tensor as an image. \"\"\"\n",
    "\n",
    "    image = tensor.to(\"cpu\").clone().detach()\n",
    "    image = image.numpy().squeeze()\n",
    "    image = image.transpose(1,2,0)\n",
    "    image = image * np.array((0.229, 0.224, 0.225)) + np.array((0.485, 0.456, 0.406))\n",
    "    image = image.clip(0, 1)\n",
    "\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(image, model, layers=None):\n",
    "    \"\"\" Run an image forward through a model and get the features for\n",
    "        a set of layers. Default layers are for VGGNet matching Gatys et al (2016)\n",
    "    \"\"\"\n",
    "\n",
    "    if layers is None:\n",
    "        layers = {'0': 'conv1_1',\n",
    "                 '5':  'conv2_1',\n",
    "                 '10': 'conv3_1',\n",
    "                 '19': 'conv4_1',\n",
    "                 '21': 'conv4_2',\n",
    "                 '28': 'conv5_1'}\n",
    "\n",
    "    features = {}\n",
    "    x = image\n",
    "    # model._modules is a dictionary holding each module in the model\n",
    "    for name, layer in model._modules.items():\n",
    "        x = layer(x)\n",
    "        if name in layers:\n",
    "            features[layers[name]] = x\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gram_matrix(tensor):\n",
    "    \"\"\" Calculate the Gram Matrix of a given tensor\n",
    "        Gram Matrix: https://en.wikipedia.org/wiki/Gramian_matrix\n",
    "    \"\"\"\n",
    "\n",
    "    gram = None\n",
    "    b, d, h, w = tensor.size()\n",
    "    tensor = tensor.view(d, h*w)\n",
    "    gram = torch.mm(tensor, tensor.t())\n",
    "    return gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get content and style features only once before forming the target image\n",
    "content_features = get_features(content, vgg)\n",
    "style_features = get_features(style, vgg)\n",
    "\n",
    "# calculate the gram matrices for each layer of our style representation\n",
    "style_grams = {layer: gram_matrix(style_features[layer]) for layer in style_features}\n",
    "\n",
    "# create a third \"target\" image and prep it for change\n",
    "# it is a good idea to start of with the target as a copy of our *content* image\n",
    "# then iteratively change its style\n",
    "target = content.clone().requires_grad_(True).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weights for each style layer\n",
    "# weighting earlier layers more will result in *larger* style artifacts\n",
    "# notice we are excluding `conv4_2` our content representation\n",
    "style_weights = {'conv1_1': 1.,\n",
    "                 'conv2_1': 0.8,\n",
    "                 'conv3_1': 0.5,\n",
    "                 'conv4_1': 0.3,\n",
    "                 'conv5_1': 0.1}\n",
    "\n",
    "content_weight = 0.0005  # alpha\n",
    "style_weight = 0.9995  # beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stylizeFunc(target, content_features):\n",
    "  # for displaying the target image, intermittently\n",
    "  show_every = 100\n",
    "\n",
    "  # iteration hyperparameters\n",
    "  optimizer = optim.Adam([target], lr=0.5)\n",
    "  steps = 100 # decide how many iterations to update your image (5000)\n",
    "  with torch.no_grad():\n",
    "    content_features = get_features(target, vgg)\n",
    "  for ii in range(1, steps+1):\n",
    "      ## Then calculate the content loss\n",
    "      target_features = get_features(target, vgg)\n",
    "      content_loss = torch.mean((target_features[\"conv4_2\"] - content_features[\"conv4_2\"]) ** 2)\n",
    "\n",
    "      # the style loss\n",
    "      # initialize the style loss to 0\n",
    "      style_loss = 0\n",
    "      # iterate through each style layer and add to the style loss\n",
    "      for layer in style_weights:\n",
    "          # get the \"target\" style representation for the layer\n",
    "          target_feature = target_features[layer]\n",
    "          target_feature = target_feature.unsqueeze(0)\n",
    "          _, d, h, w = target_feature.shape\n",
    "          target_gram = gram_matrix(target_feature)\n",
    "          style_gram = style_grams[layer]\n",
    "          layer_style_loss = style_weights[layer] * torch.mean((target_gram - style_gram) ** 2)\n",
    "\n",
    "          # add to the style loss\n",
    "          style_loss += layer_style_loss / (d * h * w)\n",
    "\n",
    "      total_loss = content_weight * content_loss + style_weight * style_loss\n",
    "\n",
    "      # update your target image\n",
    "      optimizer.zero_grad()\n",
    "      total_loss.backward()\n",
    "      optimizer.step()\n",
    "\n",
    "      # display intermediate images and print the loss\n",
    "      if  ii % show_every == 0 or ii == steps:\n",
    "          print('',end='')\n",
    "          # print('Total loss: ', total_loss.item())\n",
    "          # plt.imshow(im_convert(target))\n",
    "          # plt.show()\n",
    "  return target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "# Load CIFAR-10 dataset\n",
    "transform = transforms.Compose([\n",
    "        transforms.Resize(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.485, 0.456, 0.406),\n",
    "                             (0.229, 0.224, 0.225))\n",
    "    ])\n",
    "cifar10 = torchvision.datasets.CIFAR10(root='data', train=False, download=True, transform=transform)\n",
    "\n",
    "# Number of images per class\n",
    "num_images_per_class = 1000\n",
    "class_images = defaultdict(list)\n",
    "\n",
    "# Select 10 images from each class\n",
    "for img, label in cifar10:\n",
    "\n",
    "    if len(class_images[label]) < num_images_per_class:\n",
    "        class_images[label].append(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stylizing images and constructing the new dataset\n",
    "stylized_images = []\n",
    "stylized_labels = []\n",
    "i = 0\n",
    "for label, images in class_images.items():\n",
    "    for img in images:\n",
    "        # Clone the image and stylize it\n",
    "        img = img.to(device)\n",
    "        target_image = img.clone().detach().requires_grad_(True).to(device)\n",
    "        content_features = get_features(img, vgg)\n",
    "        stylized_image = stylizeFunc(target_image, content_features)\n",
    "\n",
    "        # Append the stylized image and its label to the dataset\n",
    "        stylized_images.append(im_convert(stylized_image.cpu()))\n",
    "        stylized_labels.append(label)\n",
    "        i += 1\n",
    "        plt.imsave(f'stylized_images/{label}/{i}.png', im_convert(stylized_image.cpu()))\n",
    "\n",
    "\n",
    "# Convert to a PyTorch tensor dataset\n",
    "stylized_dataset = list(zip(stylized_images, stylized_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(224, 224, 3)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "stylized_images[0].shape"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
